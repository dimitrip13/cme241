{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change notebook width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List, Optional, Iterable, Mapping, Sequence\n",
    "from rl.markov_decision_process import *\n",
    "from rl.markov_process import *\n",
    "from rl.distribution import *\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy\n",
    "from rl.approximate_dynamic_programming import back_opt_qvf\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskyDistribution(SampledDistribution[Tuple[float, float]]):\n",
    "    \"\"\"\n",
    "    A class container for the risky returns distribution.\n",
    "    Super's into the SampledDistribution class with a function sampler and expectation values.\n",
    "    Since we know the Risky Return Distribution we can calculate it directly instead\n",
    "    of taking a bunch of sequential samples\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma, reward):\n",
    "        # Get mu, sigma, and reward\n",
    "        self.mu:     float = mu\n",
    "        self.sigma:  float = sigma\n",
    "        self.reward: float = reward\n",
    "            \n",
    "        # Create a sampler of a tuple of a random normal value with \n",
    "        # mean mu and variance sigma coupled with a reward\n",
    "        dis_sampler = lambda : (np.random.normal(mu, sigma), reward)\n",
    "        \n",
    "        # Feed the sampler and one single sample to take to the SampledDistribution class\n",
    "        super().__init__(sampler=dis_sampler,  expectation_samples=1)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Distribution[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Distribution[float]\n",
    "\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(set(self.risky_alloc_choices))\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                wealth: float,\n",
    "                alloc: float\n",
    "# Changed Range vv\n",
    "            ) -> RiskyDistribution:\n",
    "                \n",
    "                # Calculate the expected wealth\n",
    "                wealth_expectation = (alloc*distr.μ) + (wealth-alloc)*(1+rate) \n",
    "                # Calculate the corresponding reward at time step t\n",
    "                reward: float = utility_f(wealth_expectation) if t == steps - 1 else 0.  \n",
    "                    \n",
    "                wealth_distribution = RiskyDistribution(wealth_expectation, distr.σ, reward)\n",
    "                    \n",
    "                return wealth_distribution\n",
    "# Changed Range ^^\n",
    "\n",
    "            def actions(self, wealth: float) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> DNNApprox[Tuple[float, float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=self.feature_functions,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> SampledDistribution[float]:\n",
    "\n",
    "        actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "        def states_sampler_func() -> float:\n",
    "            wealth: float = self.initial_wealth_distribution.sample()\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                wealth = alloc * (1 + distr.sample()) + \\\n",
    "                    (wealth - alloc) * (1 + rate)\n",
    "            return wealth\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_qvf(self) -> \\\n",
    "            Iterator[DNNApprox[Tuple[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[float, float]] = self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[Tuple[float, float]],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "    def get_vf_func_approx(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> DNNApprox[float]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ff,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[float], float]]\n",
    "    ) -> Iterator[Tuple[DNNApprox[float], Policy[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[float] = self.get_vf_func_approx(ff)\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[float],\n",
    "            SampledDistribution[float]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-8\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction on Q-Value Function\n",
      "--------------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -19.362\n",
      "Optimal Weights below:\n",
      "array([[-3.07409814,  1.37970284, -1.31129987,  0.04238005]])\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -6.263\n",
      "Optimal Weights below:\n",
      "array([[-2.06243504,  1.26386913, -1.01412592, -0.02190888]])\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -2.289\n",
      "Optimal Weights below:\n",
      "array([[-0.90812639,  1.13310174, -1.08980338,  0.03676592]])\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.000, Opt Val = -0.878\n",
      "Optimal Weights below:\n",
      "array([[-3.00438357e-04,  1.07000010e+00, -9.39598087e-01,\n",
      "        -1.29889411e-04]])\n",
      "\n",
      "Analytical Solution\n",
      "-------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.224, Opt Val = -0.225\n",
      "Bias Weight = 0.135\n",
      "W_t Weight = 1.311\n",
      "x_t Weight = 0.074\n",
      "x_t^2 Weight = -0.030\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.310, Opt Val = -0.257\n",
      "Bias Weight = 0.090\n",
      "W_t Weight = 1.225\n",
      "x_t Weight = 0.069\n",
      "x_t^2 Weight = -0.026\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.402, Opt Val = -0.291\n",
      "Bias Weight = 0.045\n",
      "W_t Weight = 1.145\n",
      "x_t Weight = 0.064\n",
      "x_t^2 Weight = -0.023\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.500, Opt Val = -0.328\n",
      "Bias Weight = 0.000\n",
      "W_t Weight = 1.070\n",
      "x_t Weight = 0.060\n",
      "x_t^2 Weight = -0.020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_var: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_var)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")\n",
    "\n",
    "# vf_ff: Sequence[Callable[[float], float]] = [lambda _: 1., lambda w: w]\n",
    "# it_vf: Iterator[Tuple[DNNApprox[float], Policy[float, float]]] = \\\n",
    "#     aad.backward_induction_vf_and_pi(vf_ff)\n",
    "\n",
    "# print(\"Backward Induction: VF And Policy\")\n",
    "# print(\"---------------------------------\")\n",
    "# print()\n",
    "# for t, (v, p) in enumerate(it_vf):\n",
    "#     print(f\"Time {t:d}\")\n",
    "#     print()\n",
    "#     opt_alloc: float = p.act(init_wealth).value\n",
    "#     val: float = v.evaluate([init_wealth])[0]\n",
    "#     print(f\"Opt Risky Allocation = {opt_alloc:.2f}, Opt Val = {val:.3f}\")\n",
    "#     print(\"Weights\")\n",
    "#     for w in v.weights:\n",
    "#         print(w.weights)\n",
    "#     print()\n",
    "\n",
    "it_qvf: Iterator[DNNApprox[Tuple[float, float]]] = aad.backward_induction_qvf()\n",
    "    \n",
    "\n",
    "print(\"Backward Induction on Q-Value Function\")\n",
    "print(\"--------------------------------------\")\n",
    "print()\n",
    "for t, q in enumerate(it_qvf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q.evaluate([(init_wealth, ac)])[0], ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0)\n",
    "    )[1]\n",
    "    val: float = max(q.evaluate([(init_wealth, ac)])[0]\n",
    "                     for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()\n",
    "\n",
    "print(\"Analytical Solution\")\n",
    "print(\"-------------------\")\n",
    "print()\n",
    "\n",
    "for t in range(steps):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    left: int = steps - t\n",
    "    growth: float = (1 + r) ** (left - 1)\n",
    "    alloc: float = base_alloc / growth\n",
    "    val: float = - np.exp(- excess * excess * left / (2 * var)\n",
    "                          - a * growth * (1 + r) * init_wealth) / a\n",
    "    bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
    "        np.log(np.abs(a))\n",
    "    w_t_wt: float = a * growth * (1 + r)\n",
    "    x_t_wt: float = a * excess * growth\n",
    "    x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
    "\n",
    "    print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "    print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "    print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "    print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "States = (Employment, Skill) where Employment = {Employed, Unemployed} and skill is a real numbered value.  In modelling the MDP, the skill can be either continuous or discrete but practically the skill level should be discrete.  A maximum skill level could also be imposed on the skill level in order to further limit the size of the statespace.  This also makes sense in real life because usually workers gain up to a certain skill level in their careers.  Unless you're a super genius or something there is no infinite skill level.  I will pick $ skill \\in [0,100] $ \n",
    "<br> <br>\n",
    "### Actions\n",
    "Actions = the fraction f spent working vs learning where f is realistically a continuous variable although in solving the MDP, f can be discretized\n",
    "$f \\in [0, 1]$ \n",
    "<br> <br>\n",
    "### Transition Probabilities\n",
    "For given employment E or U and given skill $s_0$ and higher skill $s_1$ and lower skill $s_{-1}$\n",
    "State transisions matched to probabilities: <br>\n",
    "{E, $s_0$} -> {U, $s_1$}     Probability of losing job = p <br>\n",
    "{E, $s_0$} -> {E, $s_1$}     Probability of not losing job = 1 - p <br>\n",
    "{U, $s_0$} -> {E, $s_{-1}$}  Probability of being offered a job = h(s) <br>\n",
    "{U, $s_0$} -> {E, $s_{-1}$}  Probability of not being offered a job = 1 - h(s) \n",
    "<br> <br>\n",
    "Given the skill decay with half life $\\lambda$,  $s(t) = s_0 e^{-\\lambda t}$ where t is the time unemployed for each day and skill increasing given by g(s)  \n",
    "$s_1 = g(s_0)$ <br>\n",
    "$s_{-1} = s_0 e^{-\\lambda t} $ \n",
    "<br> <br>\n",
    "### Rewards\n",
    "If employed R = f(s) * hours working <br>\n",
    "If unemployed R = 0\n",
    "<br> <br>\n",
    "\n",
    "### Variations\n",
    "For a finite horizon vs infinite horizon, initially the policy should be the same until close to the end of the horizon where I would guess the policy of the finite horizon would choose to work more and learn less to maximize the wages before the end of the simulation.  \n",
    "<br> <br>\n",
    "For multiple skills, the state tuple would get bigger making the state space much larger, and making the MDP harder to solve.  The action space would also change to a tuple of the fraction of (employment, skill_1, skill_2, ... skill_n-1) since you can figure out the fraction of skill_n based on the other skills.  The reward model would also change since the wage function f could pay differing amounts for different skills potentially prioritizing certain skills over others. Learning certain skills would also affect the types of jobs offered.\n",
    "<br> <br>\n",
    "For multiple jobs you introduce the possibility of accepting or rejecting different jobs with different wage reward models, adding another action object to keep track of.  \n",
    "<br> <br>\n",
    "Adding consumption costs could add a fixed negative reward for being both employed and unemployed. There would also be an added penalty for consuming more than the total wages you have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
