{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change notebook width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List, Optional, Iterable, Mapping, Sequence, Callable, Iterator\n",
    "from rl.markov_decision_process import *\n",
    "from rl.markov_process import *\n",
    "from rl.distribution import *\n",
    "from rl.dynamic_programming import *\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State is a tuple (t,S,W,I)\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    t: float # Time step\n",
    "    S: float # Order Book Mid Price at time t\n",
    "    W: float # PnL - Prices and Losses at time t.  PnL = Value Today - Value from Prior Day \n",
    "    I: float # Inventory (can be + or -) at time t\n",
    "\n",
    "\n",
    "\n",
    "#An action is a Tuple(bid,ask)\n",
    "@dataclass(frozen=True)    \n",
    "class Action:\n",
    "    Pb: float # Bid Price at time t\n",
    "    Pa: float # Ask Price at time t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Equation Assumed from Book pg 312\n",
    "# Given an an input and gamma, gets the utility\n",
    "def utility(x, γ) -> float:\n",
    "    return -np.exp(-γ*x)\n",
    "\n",
    "# Calculates the market maker's Bid Spread (relative to the order book mid)\n",
    "def calc_δ_b(state, T, γ, σ, k):\n",
    "    return (.5 * (2*state.I + 1) * γ * σ**2 * (T-state.t)) + ((1/γ) * np.log(1 + γ/k))\n",
    "\n",
    "# Calculates the market maker's Ask Spread (relative to the order book mid)\n",
    "def calc_δ_a(state, T, γ, σ, k):\n",
    "    return (.5 * (1 - 2*state.I) * γ * σ**2 * (T-state.t)) + ((1/γ) * np.log(1 + γ/k))\n",
    "\n",
    "# Returns an array of rewards, inventory, bids, and asks\n",
    "def process_dat_trace(trace, num_traces):\n",
    "    # Initialize empty arrays\n",
    "    reward_arr = []\n",
    "    inventory_arr = []\n",
    "    bid_arr = []\n",
    "    ask_arr = []\n",
    "    \n",
    "    # Loop over each trace\n",
    "    for count, simulation in enumerate(trace):\n",
    "        if count > num_traces:\n",
    "            break\n",
    "            \n",
    "        # Get the last item from the simulation\n",
    "        for item in simulation:\n",
    "            reward = item.reward\n",
    "            inventory = item.state.I\n",
    "            bid = item.action.Pb\n",
    "            ask = item.action.Pa\n",
    "            \n",
    "        # Append values to arrays\n",
    "        reward_arr.append(reward)\n",
    "        inventory_arr.append(inventory)\n",
    "        bid_arr.append(bid)\n",
    "        ask_arr.append(ask)\n",
    "        \n",
    "    return reward_arr, inventory_arr, bid_arr, ask_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Policy with a constant Bid-Ask Spread = Average of the Optimal Policy\n",
    "# The Naive Policy is always symmetric around the Mid Price\n",
    "class NaivePolicy(Policy[State,Action]):\n",
    "    T: float\n",
    "    γ: float\n",
    "    σ: float\n",
    "    k: float\n",
    "    spread:float\n",
    "    \n",
    "    def __init__(self, Τ, γ, σ, k, spread):\n",
    "        self.T = T\n",
    "        self.γ = γ\n",
    "        self.σ = σ\n",
    "        self.k = k\n",
    "        self.spread = spread\n",
    "    def act(self, state: State) -> Optional[Distribution[Action]]:\n",
    "        Pb: float = state.S - self.spread/2\n",
    "        Pa: float = state.S + self.spread/2\n",
    "        return Constant(Action(Pb, Pa))\n",
    "\n",
    "\n",
    "    \n",
    "# Optimal Policy Derived from class Symmetric around the Indifferent Price\n",
    "class OptimalPolicy(Policy[State, Action]):\n",
    "    T: float\n",
    "    γ: float\n",
    "    σ: float\n",
    "    k: float\n",
    "    \n",
    "    def __init__(self, Τ, γ, σ, k):\n",
    "        self.T = T\n",
    "        self.γ = γ\n",
    "        self.σ = σ\n",
    "        self.k = k\n",
    "        \n",
    "    def act(self, state: State) -> Optional[Distribution[Action]]:\n",
    "        δ_b: float = calc_δ_b(state, self.T, self.γ, self.σ, self.k)\n",
    "        δ_a: float = calc_δ_a(state, self.T, self.γ, self.σ, self.k)\n",
    "        Pb:  float = state.S - δ_b\n",
    "        Pa:  float = state.S + δ_a\n",
    "        return Constant(Action(Pb, Pa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market_Making_Problem_MDP(MarkovDecisionProcess[State,Action]):    \n",
    "    T:  float\n",
    "    δt: float\n",
    "    γ:  float\n",
    "    σ:  float\n",
    "    k:  float\n",
    "    c:  float\n",
    "    naive_spread: float\n",
    "        \n",
    "    def __init__(self, T, δt, γ, σ, k, c, naive_spread):\n",
    "        self.T:  float = T\n",
    "        self.δt: float = δt\n",
    "        self.γ:  float = γ\n",
    "        self.σ:  float = σ\n",
    "        self.k:  float = k\n",
    "        self.c:  float = c\n",
    "        self.naive_spread = naive_spread\n",
    "        \n",
    "    def actions(self, state: State) -> Iterable[Action]:\n",
    "        action_list: Iterable[Action] = [] # Define empty list\n",
    "            \n",
    "        # Actions for Naive Policy\n",
    "        Pb:float = state.S - self.naive_spread/2\n",
    "        Pa:float = state.S + self.naive_spread/2\n",
    "        action_list.append(Action(Pb, Pa))\n",
    "        \n",
    "        # Actions for Optimal Policy\n",
    "        δ_b: float = calc_δ_b(state, self.T, self.γ, self.σ, self.k)\n",
    "        δ_a: float = calc_δ_a(state, self.T, self.γ, self.σ, self.k)\n",
    "        Pb:  float = state.S - δ_b\n",
    "        Pa:  float = state.S + δ_a\n",
    "        action_list.append(Action(Pb, Pa))\n",
    "        \n",
    "        return list_actions\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        state: State,\n",
    "        action: Action\n",
    "                                ) -> Optional[Distribution[Tuple[State, float]]]:\n",
    "        if state.t > self.T:\n",
    "            return None  \n",
    "        \n",
    "        def my_mega_sampler() -> Tuple[State, float]:\n",
    "            # Setup basic variables\n",
    "            PnL = state.W\n",
    "            Inventory = state.I\n",
    "            \n",
    "            # Get Market Maker's Bid and Ask Spread\n",
    "            δb = state.S - action.Pb\n",
    "            δa = action.Pa - state.S\n",
    "            \n",
    "            # Get probabilities of Incrementing and Decrementing Inventory\n",
    "            prob_inventory_increment: float = self.c * self.δt * np.exp(-self.k*δb)\n",
    "            prob_inventory_decrement: float = self.c * self.δt * np.exp(-self.k*δa)\n",
    "            \n",
    "            # Increment or Decrement Inventory based on probabilities given\n",
    "            if Inventory >= 1 and np.random.random() < prob_inventory_decrement:\n",
    "                PnL += action.Pa\n",
    "                Inventory -=1\n",
    "            if np.random.random() < prob_inventory_increment:\n",
    "                PnL -= action.Pb\n",
    "                Inventory +=1       \n",
    "                \n",
    "            # Update Order Book Mid Price\n",
    "            order_book_mid_price = state.S\n",
    "            if np.random.random() < 0.5:\n",
    "                order_book_mid_price += self.σ*np.sqrt(self.δt)\n",
    "            else:\n",
    "                order_book_mid_price -= self.σ*np.sqrt(self.δt)\n",
    "            \n",
    "            # Set next time step and Next State\n",
    "            next_t = state.t + self.δt\n",
    "            next_state = State(t=next_t, S=order_book_mid_price, W=PnL, I=Inventory)\n",
    "                               \n",
    "            # Get Reward depending on if the simulation is completed\n",
    "            if next_state.t >= self.T:\n",
    "                reward = utility(next_state.W + next_state.I*next_state.S, self.γ)\n",
    "            else:\n",
    "                reward = 0\n",
    "            \n",
    "            return (next_state, reward)\n",
    "        \n",
    "        return SampledDistribution(sampler=my_mega_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive\n",
      "Reward:    \t -1.0\n",
      "Inventory: \t 0.0\n",
      "Bid Price: \t 50.002988621044395\n",
      "Ask Price: \t 149.94812847260386\n",
      "\n",
      "Optimal\n",
      "Reward:    \t -0.8826705703552483\n",
      "Inventory: \t 0.9400599400599401\n",
      "Bid Price: \t 99.29875464018373\n",
      "Ask Price: \t 100.59152506293516\n"
     ]
    }
   ],
   "source": [
    "# Set Initial Variables\n",
    "S = 100\n",
    "T = 1\n",
    "δt = 0.005\n",
    "γ = 0.1\n",
    "σ = 2\n",
    "I = 0\n",
    "k = 1.5\n",
    "c = 140\n",
    "num_traces = 1000\n",
    "\n",
    "# Set start state and distribution\n",
    "t = 0\n",
    "S = S\n",
    "W = 0\n",
    "I = I\n",
    "start_state = State(t, S, W, I)        \n",
    "start_state_distribution = Constant(start_state)\n",
    "\n",
    "# Setup Optimal Model and Policy\n",
    "optimal_model = Market_Making_Problem_MDP(T, δt, γ, σ, I, k, c)\n",
    "optimal_policy = OptimalPolicy(T, γ, σ, k)\n",
    "\n",
    "# Get Optimal Trace and Simulation Arrays\n",
    "optimal_traces = naive_model.action_traces(start_state_distribution, optimal_policy)\n",
    "optimal_reward, optimal_inventory, optimal_bids, optimal_asks = process_dat_trace(optimal_traces, num_traces)\n",
    "\n",
    "# Set Naive Spread as average of bids and asks from optimal trace\n",
    "naive_spread = np.mean(optimal_bids + optimal_asks)\n",
    "\n",
    "# Setup Naive Model and Policy\n",
    "naive_model = Market_Making_Problem_MDP(T, δt, γ, σ, I, k, c)\n",
    "naive_policy =  NaivePolicy(T, γ, σ, k, naive_spread)\n",
    "\n",
    "# Get Naive Trace and Simulation Arrays\n",
    "naive_traces = new_model.action_traces(start_state_distribution, naive_policy)\n",
    "naive_reward, naive_inventory, naive_bids, naive_asks = process_dat_trace(naive_traces, num_traces)\n",
    "\n",
    "print(\"Naive\")\n",
    "print(\"Reward:    \\t\", np.mean(naive_reward))\n",
    "print(\"Inventory: \\t\", np.mean(naive_inventory))\n",
    "print(\"Bid Price: \\t\", np.mean(naive_bids))\n",
    "print(\"Ask Price: \\t\", np.mean(naive_asks))\n",
    "print()\n",
    "print(\"Optimal\")\n",
    "print(\"Reward:    \\t\", np.mean(optimal_reward))\n",
    "print(\"Inventory: \\t\", np.mean(optimal_inventory))\n",
    "print(\"Bid Price: \\t\", np.mean(optimal_bids))\n",
    "print(\"Ask Price: \\t\", np.mean(optimal_asks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
