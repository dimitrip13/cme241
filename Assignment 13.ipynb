{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change notebook width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "from rl.markov_decision_process import *\n",
    "from rl.markov_process import *\n",
    "from rl.distribution import *\n",
    "from rl.dynamic_programming import *\n",
    "from rl.returns import *\n",
    "from scipy.stats import poisson\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "from rl.monte_carlo import mc_prediction\n",
    "from rl.td import td_prediction\n",
    "from rl.function_approx import Tabular\n",
    "from rl.function_approx import FunctionApprox\n",
    "from rl.markov_process import FiniteMarkovRewardProcess\n",
    "import matplotlib.pyplot as plt\n",
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.on_hand == other.on_hand) and  (self.on_order == other.on_order)\n",
    "\n",
    "\n",
    "class SimpleInventoryMRP(MarkovRewardProcess[InventoryState]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "    def transition_reward(\n",
    "        self,\n",
    "        state: InventoryState\n",
    "    ) -> SampledDistribution[Tuple[InventoryState, float]]:\n",
    "\n",
    "        def sample_next_state_reward(state=state) ->\\\n",
    "                Tuple[InventoryState, float]:\n",
    "            demand_sample: int = np.random.poisson(self.poisson_lambda)\n",
    "            ip: int = state.inventory_position()\n",
    "            next_state: InventoryState = InventoryState(\n",
    "                max(ip - demand_sample, 0),\n",
    "                max(self.capacity - ip, 0)\n",
    "            )\n",
    "            reward: float = - self.holding_cost * state.on_hand\\\n",
    "                - self.stockout_cost * max(demand_sample - ip, 0)\n",
    "            return next_state, reward\n",
    "\n",
    "        return SampledDistribution(sample_next_state_reward)\n",
    "\n",
    "\n",
    "class SimpleInventoryMRPFinite(FiniteMarkovRewardProcess[InventoryState]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_transition_reward_map())\n",
    "\n",
    "    def get_transition_reward_map(self) -> RewardTransition[InventoryState]:\n",
    "        d: Dict[InventoryState, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state = InventoryState(alpha, beta)\n",
    "                ip = state.inventory_position()\n",
    "                beta1 = self.capacity - ip\n",
    "                base_reward = - self.holding_cost * state.on_hand\n",
    "                sr_probs_map: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                    {(InventoryState(ip - i, beta1), base_reward):\n",
    "                     self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "                probability = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                reward = base_reward - self.stockout_cost *\\\n",
    "                    (probability * (self.poisson_lambda - ip) +\n",
    "                     ip * self.poisson_distr.pmf(ip))\n",
    "                sr_probs_map[(InventoryState(0, beta1), reward)] = probability\n",
    "                d[state] = Categorical(sr_probs_map)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TypeVar('S')\n",
    "A = TypeVar('A')\n",
    "V = Mapping[S, float]\n",
    "Q = Mapping[Tuple[S, A], float]\n",
    "\n",
    "def initiate_SARSA(mdp):\n",
    "    # Init Dicts\n",
    "    q: Q = {}\n",
    "    actions: Mapping[InventoryState, List[int]] = {}\n",
    "    counts_per_state_action: Mapping[Tuple[S, A], int] = {}\n",
    "    policy_map: Mapping[S, Optional[Categorical[A]]] = {}\n",
    "    \n",
    "    # Get States\n",
    "    states:List[InventoryState] = mdp.non_terminal_states\n",
    "    \n",
    "    # Populate actions\n",
    "    for i in mdp.mapping:\n",
    "        actions[i] = list(mdp.mapping[i].keys()) \n",
    "    \n",
    "    # Loop over states\n",
    "    for state in states:\n",
    "        # Set counts to zero for all state action pairs\n",
    "        for action in actions[state]:\n",
    "            q[(state, action)] = 0.\n",
    "            counts_per_state_action[(state, action)] = 0\n",
    "            \n",
    "        # Setup policy map\n",
    "        if actions[state] is None:\n",
    "            policy_map[state] = None\n",
    "        else:\n",
    "            policy_map[state] = Categorical({action:1 for action in actions[state]})\n",
    "    \n",
    "    # Get Policy from policy map\n",
    "    Policy: FinitePolicy[S, A] = FinitePolicy(policy_map)\n",
    "        \n",
    "    return states, actions, q, counts_per_state_action, Policy\n",
    "\n",
    "\n",
    "def get_Q_vf_π(q :Q) -> Tuple[V, FinitePolicy[S,A]]:\n",
    "        # Init Dicts\n",
    "        vf: V = {}\n",
    "        policy_map: Mapping[S, Optional[Constant[A]]] = {}\n",
    "        \n",
    "        # Loop over state and action of Q function\n",
    "        for state, action in q:\n",
    "            if state not in vf.keys() or q[(state, action)] > vf[state]:\n",
    "                vf[state] = q[(state, action)]\n",
    "                policy_map[state] = Constant(action)\n",
    "        Policy = FinitePolicy(policy_map)\n",
    "        \n",
    "        return (vf,Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_sarsa(\n",
    "        mdp:FiniteMarkovDecisionProcess,\n",
    "        states: List[S],\n",
    "        actions:Mapping[S,List[A]],\n",
    "        γ: float,\n",
    "        q: Q,\n",
    "        counts :Mapping[Tuple[S, A], int],\n",
    "        policy :FinitePolicy[S, A],\n",
    "        ε:float = 0.1,\n",
    "        αi:float = 0.05,\n",
    "        num_episodes:float = 10000,\n",
    "        half_life:float = 1000.0,\n",
    "        exponent:float = 0.5\n",
    "                                                                                            ) ->V:\n",
    "    # Get initial state sample\n",
    "    state = Categorical({state:1 for state in states}).sample()\n",
    "    for i in range(num_episodes):\n",
    "        # Get Action\n",
    "        action = policy.act(state).sample()\n",
    "        # Get Next State and Reward\n",
    "        next_state, reward = mdp.step(state, action).sample()\n",
    "        # Get Next Action\n",
    "        next_action = policy.act(next_state).sample()\n",
    "        \n",
    "        # Increment state action count\n",
    "        counts[(state, action)] += 1\n",
    "        # Get Learning rate\n",
    "        α = αi / (1 + ((counts[(state, action)] -1) / half_life)**exponent)\n",
    "        #Choose the next action based on epsilon greedy policy\n",
    "        q[(state,action)] += α * (reward + γ*q[(next_state, next_action)] - q[(state, action)])\n",
    "        \n",
    "\n",
    "        # Update if at terminal state\n",
    "        new_policy = policy.policy_map\n",
    "        if actions[state] is None:\n",
    "            new_policy[state] = None\n",
    "            \n",
    "        # Update new policy\n",
    "        policy_map = {action: ε / len(actions[state]) for action in actions[state]}\n",
    "        optimal_action = actions[state][np.argmax([q[(state, action)] for action in actions[state]])]\n",
    "        policy_map[optimal_action] += 1 - ε\n",
    "        new_policy[state] = Categorical(policy_map)\n",
    "        policy = FinitePolicy(new_policy)\n",
    "        \n",
    "        # Set state for next iteration\n",
    "        state = next_state\n",
    "        \n",
    "        # Start over with new sample if next state is a terminal state\n",
    "        if next_state is None:\n",
    "            state = Categorical({state:1 for state in states}).sample()\n",
    "            \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ye Old MDP Value Iteration\n",
      "{InventoryState(on_hand=0, on_order=0): -34.89484576629397,\n",
      " InventoryState(on_hand=1, on_order=0): -28.660950216301437,\n",
      " InventoryState(on_hand=0, on_order=2): -27.991890076067463,\n",
      " InventoryState(on_hand=0, on_order=1): -27.66095021630144,\n",
      " InventoryState(on_hand=1, on_order=1): -28.991890076067467,\n",
      " InventoryState(on_hand=2, on_order=0): -29.991890076067463}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n",
      "SARSA\n",
      "\n",
      "{InventoryState(on_hand=0, on_order=0): -35.31084044326533,\n",
      " InventoryState(on_hand=1, on_order=0): -29.19100428777324,\n",
      " InventoryState(on_hand=0, on_order=2): -28.86555150285234,\n",
      " InventoryState(on_hand=0, on_order=1): -28.030243892190832,\n",
      " InventoryState(on_hand=1, on_order=1): -29.289110445880922,\n",
      " InventoryState(on_hand=2, on_order=0): -29.92241168103019}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=1):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=0, on_order=2):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=0):\n",
      "  Do Action 1 with Probability 1.000\n",
      "For State InventoryState(on_hand=1, on_order=1):\n",
      "  Do Action 0 with Probability 1.000\n",
      "For State InventoryState(on_hand=2, on_order=0):\n",
      "  Do Action 0 with Probability 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "γ = 0.9\n",
    "\n",
    "mdp: FiniteMarkovDecisionProcess[InventoryState, int] = SimpleInventoryMDPCap(\n",
    "                capacity=user_capacity,\n",
    "                poisson_lambda=user_poisson_lambda,\n",
    "                holding_cost=user_holding_cost,\n",
    "                stockout_cost=user_stockout_cost\n",
    "    )\n",
    "    \n",
    "print(\"Ye Old MDP Value Iteration\")\n",
    "opt_vf_vi, opt_policy_vi = value_iteration_result(mdp, gamma=γ)\n",
    "pprint(opt_vf_vi)\n",
    "print()\n",
    "print(opt_policy_vi)\n",
    "print()\n",
    "\n",
    "\n",
    "states, actions, q, counts, Policy = initiate_SARSA(mdp)\n",
    "\n",
    "print(\"SARSA\")\n",
    "q_sarsa = tabular_sarsa(mdp,\n",
    "                        states,\n",
    "                        actions,\n",
    "                        γ,\n",
    "                        q,\n",
    "                        counts,\n",
    "                        Policy)\n",
    "\n",
    "sarsa_vf, sarsa_policy = get_Q_vf_π(q_sarsa)\n",
    "print()\n",
    "pprint(sarsa_vf)\n",
    "print()\n",
    "pprint(sarsa_policy)\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
